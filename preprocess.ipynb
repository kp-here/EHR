{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gp0RLkh9BryW","executionInfo":{"status":"ok","timestamp":1681899649320,"user_tz":-330,"elapsed":52583,"user":{"displayName":"kp-here","userId":"12498103533326523955"}},"outputId":"86106435-8c77-4ac7-d37a-3b52abcb9dd1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"eUbmLEYwBryi","executionInfo":{"status":"ok","timestamp":1681899702818,"user_tz":-330,"elapsed":669,"user":{"displayName":"kp-here","userId":"12498103533326523955"}}},"outputs":[],"source":["import sys\n","sys.path.append('../')\n","import numpy as np\n","import pandas as pd\n","from collections import Counter,defaultdict\n","import csv\n","import math\n","import operator"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"jh_4D7e0Bryk","executionInfo":{"status":"ok","timestamp":1681899706447,"user_tz":-330,"elapsed":624,"user":{"displayName":"kp-here","userId":"12498103533326523955"}}},"outputs":[],"source":["# MODEL_DIR = '/content/drive/MyDrive/OUT-K3/MODELS'\n","# VOCAB_DIR = '/content/drive/MyDrive/OUT-K3/VOCAB'\n","# INPUT_DIR='/content/drive/MyDrive/EHR/DATASET'\n","# DATA_DIR = '/content/drive/MyDrive/EHR/DATA'\n","# MIMIC_3_DIR = '/content/drive/MyDrive/OUT-K3/MIMIC3'\n","\n","# MODEL_DIR = '/content/drive/MyDrive/OUT-K3/MODELS'\n","# VOCAB_DIR = '/content/drive/MyDrive/OUT-K3/VOCAB'\n","INPUT_DIR='/content/drive/MyDrive/EHR-GIT/DATASET'\n","# DATA_DIR = '/content/drive/MyDrive/EHR/DATA'\n","MIMIC_3_DIR = '/content/drive/MyDrive/EHR-GIT/MIMIC3'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NDl2P-4jBrym","outputId":"9136cc83-a3d8-44a6-cdce-0de7a6a03521"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/drive/MyDrive/OUT-K3\n"]}],"source":["cd /content/drive/MyDrive/OUT-K3"]},{"cell_type":"markdown","metadata":{"id":"szayriz_Bryq"},"source":["NOTEEVENTS file contains clinical notes of the patient along with other details like subject_id, hadm_id, category of the clinical notes etc."]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GLIV8wFlBryx","executionInfo":{"status":"ok","timestamp":1681899802558,"user_tz":-330,"elapsed":85392,"user":{"displayName":"kp-here","userId":"12498103533326523955"}},"outputId":"90a89572-475c-4bf1-8e7f-1e396dc1beea"},"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-4-30671e7fae59>:1: DtypeWarning: Columns (4,5) have mixed types. Specify dtype option on import or set low_memory=False.\n","  df = pd.read_csv('%s/NOTEEVENTS.csv' % INPUT_DIR)\n"]}],"source":["df = pd.read_csv('%s/NOTEEVENTS.csv' % INPUT_DIR)"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":337},"id":"5DTM7iOUBryy","executionInfo":{"status":"ok","timestamp":1681899899329,"user_tz":-330,"elapsed":1183,"user":{"displayName":"kp-here","userId":"12498103533326523955"}},"outputId":"c77380ae-6885-4d7f-b646-a247a33839bb"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["   ROW_ID  SUBJECT_ID   HADM_ID   CHARTDATE CHARTTIME STORETIME  \\\n","0     174       22532  167853.0  2151-08-04       NaN       NaN   \n","1     175       13702  107527.0  2118-06-14       NaN       NaN   \n","2     176       13702  167118.0  2119-05-25       NaN       NaN   \n","3     177       13702  196489.0  2124-08-18       NaN       NaN   \n","4     178       26880  135453.0  2162-03-25       NaN       NaN   \n","\n","            CATEGORY DESCRIPTION  CGID  ISERROR  \\\n","0  Discharge summary      Report   NaN      NaN   \n","1  Discharge summary      Report   NaN      NaN   \n","2  Discharge summary      Report   NaN      NaN   \n","3  Discharge summary      Report   NaN      NaN   \n","4  Discharge summary      Report   NaN      NaN   \n","\n","                                                TEXT  \n","0  Admission Date:  [**2151-7-16**]       Dischar...  \n","1  Admission Date:  [**2118-6-2**]       Discharg...  \n","2  Admission Date:  [**2119-5-4**]              D...  \n","3  Admission Date:  [**2124-7-21**]              ...  \n","4  Admission Date:  [**2162-3-3**]              D...  "],"text/html":["\n","  <div id=\"df-1fa41d69-70e8-41e8-9483-f4caffa56830\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>ROW_ID</th>\n","      <th>SUBJECT_ID</th>\n","      <th>HADM_ID</th>\n","      <th>CHARTDATE</th>\n","      <th>CHARTTIME</th>\n","      <th>STORETIME</th>\n","      <th>CATEGORY</th>\n","      <th>DESCRIPTION</th>\n","      <th>CGID</th>\n","      <th>ISERROR</th>\n","      <th>TEXT</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>174</td>\n","      <td>22532</td>\n","      <td>167853.0</td>\n","      <td>2151-08-04</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Discharge summary</td>\n","      <td>Report</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Admission Date:  [**2151-7-16**]       Dischar...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>175</td>\n","      <td>13702</td>\n","      <td>107527.0</td>\n","      <td>2118-06-14</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Discharge summary</td>\n","      <td>Report</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Admission Date:  [**2118-6-2**]       Discharg...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>176</td>\n","      <td>13702</td>\n","      <td>167118.0</td>\n","      <td>2119-05-25</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Discharge summary</td>\n","      <td>Report</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Admission Date:  [**2119-5-4**]              D...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>177</td>\n","      <td>13702</td>\n","      <td>196489.0</td>\n","      <td>2124-08-18</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Discharge summary</td>\n","      <td>Report</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Admission Date:  [**2124-7-21**]              ...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>178</td>\n","      <td>26880</td>\n","      <td>135453.0</td>\n","      <td>2162-03-25</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Discharge summary</td>\n","      <td>Report</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Admission Date:  [**2162-3-3**]              D...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1fa41d69-70e8-41e8-9483-f4caffa56830')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-1fa41d69-70e8-41e8-9483-f4caffa56830 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-1fa41d69-70e8-41e8-9483-f4caffa56830');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":5}],"source":["df.head()"]},{"cell_type":"markdown","metadata":{"id":"v7FxrZmEBryz"},"source":["Preprocess the text - remove punctuation, convert to lowercase, remove less important words and remove unnecessary sections like medications"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vpGozxF_Bry0"},"outputs":[],"source":["\n","from nltk.tokenize import RegexpTokenizer\n","\n","tokenizer = RegexpTokenizer(r'\\w+')\n","\n","with open('%s/NOTEEVENTS.csv' % INPUT_DIR, 'r') as infile:\n","  with open('%s/NOTES_FILTERED.csv' % MIMIC_3_DIR, 'w') as outfile:\n","    outfile.write(','.join(['SUBJECT_ID', 'HADM_ID', 'CHARTDATE', 'CHARTTIME', 'STORETIME', 'DESCRIPTION', 'CGID', 'ISERROR', 'TEXT']) + '\\n')\n","    reader=csv.reader(infile)\n","    next(reader)  \n","    count=0  \n","    flag=0\n","    \n","    for row in reader:\n","      text_new=\" \"\n","      flag=flag+1     \n","      if row[6]=='Discharge summary':\n","        text = row[10]\n","        for line in text.split('\\n\\n'):\n","          tokens = [t.lower() for t in tokenizer.tokenize(line) if (t.isnumeric()==False and len(t)>3 and t.lower() not in['this','also','date','birth','admission','discharge','studies','name'] )]\n","          # print(tokens)\n","          # l1=[\"medications\",\"admission\"]\n","          if (\"medications\" in tokens and (\"discharge\" in tokens or \"admission\" in tokens)):\n","            continue\n","          else:\n","            # print(tokens)\n","            new_text=  ' '.join(tokens) \n","            text_new+=new_text+' '\n","        # print(text_new)\n","        outfile.write(','.join([ row[1], row[2], row[3], row[4], row[5], row[7], row[8], row[9], text_new]) + '\\n') \n","        # if(flag==2):\n","        #   break\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2xHH7y-IBry4"},"outputs":[],"source":["dfl = pd.read_csv('%s/NOTES_FILTERED.csv' % MIMIC_3_DIR)\n","dfl.head()"]},{"cell_type":"markdown","metadata":{"id":"V0Hck6nDBry5"},"source":["Preprocess the data to extract the necessary information from the dataset.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yVgQWUvtBry7"},"outputs":[],"source":["df = pd.read_csv('%s/NOTES_FILTERED.csv' % MIMIC_3_DIR)\n","hadm_ids = set(df['HADM_ID'])\n","with open('%s/DIAGNOSES_ICD.csv' % INPUT_DIR, 'r') as lf:\n","    with open('%s/CODES_FILTERED.csv' % MIMIC_3_DIR , 'w') as of:\n","        w = csv.writer(of)\n","        w.writerow(['SUBJECT_ID', 'HADM_ID', 'ICD9_CODE'])\n","        r = csv.reader(lf)\n","        #header\n","        next(r)\n","        for i,row in enumerate(r):\n","            hadm_id = int(row[2])\n","            #print(hadm_id)\n","            #break\n","            if hadm_id in hadm_ids:\n","                w.writerow(row[1:3] + [row[4]])"]},{"cell_type":"markdown","metadata":{"id":"O7AbuPahBry-"},"source":["Sort the filtered NOTES and CODES"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3xINTZKeBrzD"},"outputs":[],"source":["# df = pd.read_csv('%s/NOTES_FILTERED.csv' % MIMIC_3_DIR)\n","df = df.sort_values(['SUBJECT_ID', 'HADM_ID'])\n","df.to_csv('%s/NOTES_FILTERED_SORTED.csv' % MIMIC_3_DIR, index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qemDMUsZBrzF"},"outputs":[],"source":["df2 = pd.read_csv('%s/CODES_FILTERED.csv' % MIMIC_3_DIR)\n","df2 = df2.sort_values(['SUBJECT_ID', 'HADM_ID'])\n","df2.to_csv('%s/CODES_FILTERED_SORTED.csv' % MIMIC_3_DIR, index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YX5O9511BrzG"},"outputs":[],"source":["def next_notes2(notesfile):\n","\n","    notes_reader = csv.reader(notesfile)\n","\n","    arr = []\n","\n","    for (i,v) in enumerate(notes_reader):\n","\n","        if(i==0):\n","            continue\n","\n","        if(i==1):\n","\n","            sid = int(v[0])\n","            hid = int(v[1])\n","            text = v[8]\n","            continue\n","    \n","        cur_sid = int(v[0])\n","        cur_hid = int(v[1])\n","        cur_text = v[8]\n","\n","        if(hid==cur_hid):\n","            text += \" \" + cur_text\n","        \n","        else:\n","            yield(sid,text,hid)\n","            hid = cur_hid\n","            sid = cur_sid\n","            text = cur_text\n","        \n","    yield(sid,text,hid)\n","\n","def next_labels2(labelsfile):\n","\n","    for (i,v) in enumerate(csv.reader(labelsfile)):\n","\n","        if(i==0):\n","            continue\n","\n","        if(i==1):\n","\n","            sid = int(v[0])\n","            hid = int(v[1])\n","            code = [v[2]]\n","            continue\n","            \n","        cur_sid = int(v[0])\n","        cur_hid = int(v[1])\n","        cur_code = v[2]\n","\n","        if(hid==cur_hid):\n","            code.append(cur_code)\n","            \n","        else:\n","            yield(sid,code,hid)\n","            hid = cur_hid\n","            sid = cur_sid\n","            code = [cur_code]\n","\n","    yield(sid,code,hid)\n","\n","def concat_data(labels, notes,out):\n","    \n","    with open(labels, 'r') as lf:\n","        print(\"CONCATENATING labels with notes\")\n","        with open(notes, 'r') as nf:\n","            with open(out, 'w') as rf:\n","                w = csv.writer(rf)\n","                w.writerow(['SUBJECT_ID', 'HADM_ID', 'TEXT', 'LABELS'])\n","\n","                labels_gen = next_labels2(lf)\n","                notes_gen = next_notes2(nf)\n","\n","                for i, (subj_id, text, hadm_id) in enumerate(notes_gen):\n","                    if i % 10000 == 0:\n","                        print(str(i) + \" done\")\n","                    cur_subj, cur_labels, cur_hadm = next(labels_gen)\n","\n","                    if cur_hadm == hadm_id:\n","                        w.writerow([subj_id, str(hadm_id), text, ' '.join(cur_labels)])\n","                    else:\n","                        print(\"Data is not sorted correctly. Cannot find matching hadm_id\")\n","                        break\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-U8OVx4lBrzI","outputId":"6b38f87c-4b28-4b6a-cf1f-2938013d8f8c"},"outputs":[{"name":"stdout","output_type":"stream","text":["CONCATENATING labels with notes\n","0 done\n","10000 done\n","20000 done\n","30000 done\n","40000 done\n","50000 done\n"]}],"source":["concat_data('%s/CODES_FILTERED_SORTED.csv' %MIMIC_3_DIR , '%s/NOTES_FILTERED_SORTED.csv' %MIMIC_3_DIR,'%s/NOTES_CODES.csv' %MIMIC_3_DIR)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6cVAoYu2BrzK"},"outputs":[],"source":["df1 = pd.read_csv('%s/NOTES_CODES.csv' %MIMIC_3_DIR)\n","df1.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2cV8pUwoBrzL"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"TNE069Q6BrzN"},"source":["### TEST Train splitting"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_jWyUZ9tBrzP"},"outputs":[],"source":["def divide(file):\n","  with open(file, 'r') as f:\n","    reader = csv.reader(f)\n","    length = len(list(reader))\n","    train_length = (length*8)//10\n","    dev_length = (length-train_length)//2\n","    test_length = length - (train_length + dev_length)\n","\n","    return length, train_length, dev_length, test_length\n","\n","\n","def split_hadm(filename):\n","  length, train_length, dev_length, test_length = divide(filename)\n","  with open(filename, 'r') as f:\n","    with open('%s/train_hadm_ids.csv' %MIMIC_3_DIR, 'w') as train_hadm_ids:\n","      with open('%s/dev_hadm_ids.csv' %MIMIC_3_DIR, 'w') as dev_hadm_ids:\n","        with open('%s/test_hadm_ids.csv' %MIMIC_3_DIR, 'w') as test_hadm_ids:\n","          train_hadm_ids.write('HADM_IDS' + \"\\n\")\n","          dev_hadm_ids.write('HADM_IDS' + \"\\n\")\n","          test_hadm_ids.write('HADMS' + \"\\n\")\n","          re = csv.reader(f)\n","          next(re)\n","          i=0\n","          for row in re:            \n","            if(i<train_length):\n","              train_hadm_ids.write(row[1] + \"\\n\")              \n","            \n","            if(i>=train_length and i<(train_length+dev_length)):\n","              dev_hadm_ids.write(row[1] + \"\\n\")              \n","\n","            if(i>=(train_length+dev_length) and i<length):\n","              test_hadm_ids.write(row[1]+ \"\\n\")\n","\n","            i+=1\n","\n","def splitdata2(filename):\n","  split_hadm(filename)\n","  train = '%s/train_discharge_split.csv' %MIMIC_3_DIR\n","  dev = '%s/dev_discharge_split.csv' %MIMIC_3_DIR\n","  test = '%s/test_discharge_split.csv' %MIMIC_3_DIR\n","  file_train = open(train, 'w')\n","  file_dev = open(dev, 'w')\n","  file_test = open(test, 'w')\n","  file_train.write(','.join(['SUBJECT_ID', 'HADM_ID', 'TEXT', 'LABELS']) + \"\\n\")\n","  file_dev.write(','.join(['SUBJECT_ID', 'HADM_ID', 'TEXT', 'LABELS']) + \"\\n\")\n","  file_test.write(','.join(['SUBJECT_ID', 'HADM_ID', 'TEXT', 'LABELS']) + \"\\n\")\n","\n","  Hadm_Ids = {}\n","\n","  for splt in ['train', 'dev', 'test']:\n","    Hadm_Ids[splt] = set()\n","    with open('%s/%s_hadm_ids.csv' %(MIMIC_3_DIR,splt), 'r') as f:\n","      for line in f:\n","        Hadm_Ids[splt].add(line.rstrip())\n","\n","  with open(filename, 'r') as fn:\n","    reader = csv.reader(fn)\n","    next(reader)\n","    i=0\n","    current_hadmid = 0\n","    for row in reader:\n","      if i % 10000 == 0:\n","        print(str(i) + \" read\")\n","      HadmId = row[1]\n","      if HadmId in Hadm_Ids['train']:\n","        file_train.write(','.join(row) + \"\\n\")\n","      elif HadmId in Hadm_Ids['dev']:\n","        file_dev.write(','.join(row) + \"\\n\")\n","      elif HadmId in Hadm_Ids['test']:\n","        file_test.write(','.join(row) + \"\\n\")\n","\n","      i += 1\n","\n","  file_train.close()\n","  file_dev.close()\n","  file_test.close()\n","\n","  return train, dev, test"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ko238bmBBrzS","outputId":"449f070e-a45a-421b-edc7-bb823a6ec07f"},"outputs":[{"name":"stdout","output_type":"stream","text":["0 read\n","10000 read\n","20000 read\n","30000 read\n","40000 read\n","50000 read\n"]}],"source":["tr, dv, te = splitdata2('%s/NOTES_CODES.csv' %MIMIC_3_DIR)\n","# print(tr)\n","# print(dv)\n","# print(te)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bS-Eeqp3BrzV"},"outputs":[],"source":["from collections import defaultdict\n","from scipy.sparse import csr_matrix\n","\n","def build_vocab(vocab_min, infile, vocab_filename):\n","    \"\"\"\n","        INPUTS:\n","            vocab_min: how many documents a word must appear in to be kept\n","            infile: (training) data file to build vocabulary from\n","            vocab_filename: name for the file to output\n","    \"\"\"\n","    with open(infile, 'r') as csvfile:\n","        reader = csv.reader(csvfile)\n","        #header\n","        next(reader)\n","\n","        #0. read in data\n","        print(\"reading in data...\")\n","        #holds number of terms in each document\n","        note_numwords = []\n","        #indices where notes start\n","        note_inds = [0]\n","        #indices of discovered words\n","        indices = []\n","        #holds a bunch of ones\n","        data = []\n","        #keep track of discovered words\n","        vocab = {}\n","        #build lookup table for terms\n","        num2term = {}\n","        #preallocate array to hold number of notes each term appears in\n","        note_occur = np.zeros(400000, dtype=int)\n","        i = 0\n","        for row in reader:\n","            text = row[2]\n","            numwords = 0\n","            for term in text.split():\n","                #put term in vocab if it's not there. else, get the index\n","                index = vocab.setdefault(term, len(vocab))\n","                indices.append(index)\n","                num2term[index] = term\n","                data.append(1)\n","                numwords += 1\n","            #record where the next note starts\n","            note_inds.append(len(indices))\n","            indset = set(indices[note_inds[-2]:note_inds[-1]])\n","            #go thru all the word indices you just added, and add to the note occurrence count for each of them\n","            for ind in indset:\n","                note_occur[ind] += 1\n","            note_numwords.append(numwords)\n","            i += 1\n","        #clip trailing zeros\n","        note_occur = note_occur[note_occur>0]\n","\n","        #turn vocab into a list so indexing doesn't get fd up when we drop rows\n","        vocab_list = np.array([word for word,ind in sorted(vocab.items(), key=operator.itemgetter(1))])\n","\n","        #1. create sparse document matrix\n","        C = csr_matrix((data, indices, note_inds), dtype=int).transpose()\n","        #also need the numwords array to be a sparse matrix\n","        note_numwords = csr_matrix(1. / np.array(note_numwords))\n","        \n","        #2. remove rows with less than 3 total occurrences\n","        print(\"removing rare terms\")\n","        #inds holds indices of rows corresponding to terms that occur in < 3 documents\n","        inds = np.nonzero(note_occur >= vocab_min)[0]\n","        print(str(len(inds)) + \" terms qualify out of \" + str(C.shape[0]) + \" total\")\n","        #drop those rows\n","        C = C[inds,:]\n","        note_occur = note_occur[inds]\n","        vocab_list = vocab_list[inds]\n","\n","        print(\"writing output\")\n","        with open(vocab_filename, 'w') as vocab_file:\n","            for word in vocab_list:\n","                vocab_file.write(word + \"\\n\")\n","\n","def build_vocab2(vocab_min, infile, vocab_filename):\n","    \"\"\"\n","        INPUTS:\n","            vocab_min: how many documents a word must appear in to be kept\n","            infile: (training) data file to build vocabulary from\n","            vocab_filename: name for the file to output\n","    \"\"\"\n","    with open(infile, 'r') as csvfile:\n","        reader = csv.reader(csvfile)\n","        #header\n","        next(reader)\n","\n","        #0. read in data\n","        print(\"reading in data...\")\n","        #holds number of terms in each document\n","        note_numwords = []\n","        #indices where notes start\n","        note_inds = [0]\n","        #indices of discovered words\n","        indices = []\n","        #holds a bunch of ones\n","        data = []\n","        #keep track of discovered words\n","        vocab = {}\n","        #build lookup table for terms\n","        num2term = {}\n","        #preallocate array to hold number of notes each term appears in\n","        note_occur = np.zeros(400000, dtype=int)\n","        i = 0\n","        for row in reader:\n","            text = row[2]\n","            numwords = 0\n","            for term in text.split():\n","                #put term in vocab if it's not there. else, get the index\n","                index = vocab.setdefault(term, len(vocab))\n","                indices.append(index)\n","                num2term[index] = term\n","                data.append(1)\n","                numwords += 1\n","            #record where the next note starts\n","            note_inds.append(len(indices))\n","            indset = set(indices[note_inds[-2]:note_inds[-1]])\n","            #go thru all the word indices you just added, and add to the note occurrence count for each of them\n","            for ind in indset:\n","                note_occur[ind] += 1\n","            note_numwords.append(numwords)\n","            i += 1\n","        #clip trailing zeros\n","        note_occur = note_occur[note_occur>0]\n","\n","        #turn vocab into a list so indexing doesn't get fd up when we drop rows\n","        vocab_list = np.array([word for word,ind in sorted(vocab.items(), key=operator.itemgetter(1))])\n","\n","        #1. create sparse document matrix\n","        C = csr_matrix((data, indices, note_inds), dtype=int).transpose()\n","        #also need the numwords array to be a sparse matrix\n","        note_numwords = csr_matrix(1. / np.array(note_numwords))\n","        \n","        #2. remove rows with less than 3 total occurrences\n","        print(\"removing rare terms\")\n","        #inds holds indices of rows corresponding to terms that occur in < 3 documents\n","        inds = np.nonzero(note_occur >= vocab_min)[0]\n","        print(str(len(inds)) + \" terms qualify out of \" + str(C.shape[0]) + \" total\")\n","        #drop those rows\n","        C = C[inds,:]\n","        note_occur = note_occur[inds]\n","        vocab_list = vocab_list[inds]\n","\n","        print(\"writing output\")\n","        with open(vocab_filename, 'w') as vocab_file:\n","            for word in vocab_list:\n","                vocab_file.write(word + \"\\n\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fiHYycMWBrzb","outputId":"7c0bbf07-a0ed-4ed7-a6f5-cf033eee60f7"},"outputs":[{"name":"stdout","output_type":"stream","text":["reading in data...\n","removing rare terms\n","43435 terms qualify out of 121309 total\n","writing output\n"]}],"source":["vocab_min = 3\n","vname = '%s/vocab.csv' %MIMIC_3_DIR\n","build_vocab2(vocab_min, tr, vname)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JteaLJZ6Brze"},"outputs":[],"source":["df=pd.read_csv('%s/vocab.csv' %MIMIC_3_DIR)\n","df.head(10)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GH_lLvNYBrzf"},"outputs":[],"source":["for splt in ['train', 'dev', 'test']:\n","    df = pd.read_csv('%s/%s_discharge_split.csv' % (MIMIC_3_DIR,splt))\n","    df['length'] = df.apply(lambda row: len(str(row['TEXT']).split()), axis=1)\n","    df = df.sort_values(['length'])\n","    df.to_csv('%s/%s_full.csv' % (MIMIC_3_DIR,splt), index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-tv-BI-mBrzg"},"outputs":[],"source":["import gensim.models.word2vec as w2v\n","\n","class ProcessedIter(object):\n","\n","    def __init__(self, Y, filename):\n","        self.filename = filename\n","\n","    def __iter__(self):\n","        with open(self.filename) as f:\n","            r = csv.reader(f)\n","            next(r)\n","            for row in r:\n","                yield (row[8].split())\n","\n","def word_embeddings(Y, notes_file, embedding_size, min_count, n_iter):\n","    # modelname = \n","    sentences = ProcessedIter(Y, notes_file)\n","\n","    model = w2v.Word2Vec(vector_size=embedding_size, min_count=min_count, workers=4, epochs=n_iter)\n","    print(\"building word2vec vocab on %s...\" % (notes_file))\n","    \n","    model.build_vocab(sentences)\n","    print(\"training...\")\n","    model.train(sentences, total_examples=model.corpus_count, epochs=model.epochs)\n","    out_file = \"%s/processed_%s.w2v\" % (MIMIC_3_DIR,Y)\n","    print(\"writing embeddings to %s\" % (out_file))\n","    model.save(out_file)\n","    return out_file"]},{"cell_type":"markdown","metadata":{"id":"3ENyCC9fBrzh"},"source":["Building w2v file from NOTES filtered\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F-Nqy5INBrzi","outputId":"3bfa581c-4b66-49f0-c108-fbfc021f7cf9"},"outputs":[{"name":"stdout","output_type":"stream","text":["building word2vec vocab on /content/drive/MyDrive/OUT-K3/MIMIC3/NOTES_FILTERED.csv...\n","training...\n","writing embeddings to /content/drive/MyDrive/OUT-K3/MIMIC3/processed_full.w2v\n"]}],"source":["w2v_file = word_embeddings('full', '%s/NOTES_FILTERED.csv' %MIMIC_3_DIR, 100, 0, 5)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-LFJpsYxBrzj","outputId":"dfb5f179-7b61-401d-9606-e948d1429c4c"},"outputs":[{"data":{"text/plain":["0.22310127"]},"metadata":{},"output_type":"display_data"}],"source":["import gensim.models\n","model = gensim.models.Word2Vec.load('%s/processed_full.w2v' %MIMIC_3_DIR)\n","wv = model.wv\n","wv.similarity('fever','cold')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zfr1iAWIBrzk"},"outputs":[],"source":["import gensim.models\n","import os\n","from tqdm import tqdm\n","\n","PAD_CHAR = \"**PAD**\"\n","\n","def gensim_to_embeddings(wv_file, vocab_file, Y, outfile=None):\n","    model = gensim.models.Word2Vec.load(wv_file)\n","    wv = model.wv\n","    #free up memory\n","    del model\n","\n","    vocab = set()\n","    with open(vocab_file, 'r') as vocabfile:\n","        for i,line in enumerate(vocabfile):\n","            line = line.strip()\n","            if line != '':\n","                vocab.add(line)\n","    ind2w = {i+1:w for i,w in enumerate(sorted(vocab))}\n","\n","    W, words = build_matrix(ind2w, wv)\n","\n","    if outfile is None:\n","        outfile = wv_file.replace('.w2v', '.embed')\n","\n","    #smash that save button\n","    save_embeddings(W, words, outfile)\n","\n","def build_matrix(ind2w, wv):\n","    \"\"\"\n","        Go through vocab in order. Find vocab word in wv.index2word, then call wv.word_vec(wv.index2word[i]).\n","        Put results into one big matrix.\n","        Note: ind2w starts at 1 (saving 0 for the pad character), but gensim word vectors starts at 0\n","    \"\"\"\n","    W = np.zeros((len(ind2w)+1, len(wv.word_vec(wv.index_to_key[0])) ))\n","    words = [PAD_CHAR]\n","    W[0][:] = np.zeros(len(wv.word_vec(wv.index_to_key[0])))\n","    for idx, word in tqdm(ind2w.items()):\n","        if idx >= W.shape[0]:\n","            break    \n","        W[idx][:] = wv.word_vec(word)\n","        words.append(word)\n","    return W, words\n","\n","def save_embeddings(W, words, outfile):\n","    with open(outfile, 'w') as o:\n","        #pad token already included\n","        for i in range(len(words)):\n","            line = [words[i]]\n","            line.extend([str(d) for d in W[i]])\n","            o.write(\" \".join(line) + \"\\n\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"USfAkX3JBrzm","outputId":"c3f02608-6763-4689-c01b-5d1a81076e81"},"outputs":[{"name":"stderr","output_type":"stream","text":["<ipython-input-11-162f7d886443>:35: DeprecationWarning: Call to deprecated `word_vec` (Use get_vector instead).\n","  W = np.zeros((len(ind2w)+1, len(wv.word_vec(wv.index_to_key[0])) ))\n","<ipython-input-11-162f7d886443>:37: DeprecationWarning: Call to deprecated `word_vec` (Use get_vector instead).\n","  W[0][:] = np.zeros(len(wv.word_vec(wv.index_to_key[0])))\n","  0%|          | 0/43435 [00:00<?, ?it/s]<ipython-input-11-162f7d886443>:41: DeprecationWarning: Call to deprecated `word_vec` (Use get_vector instead).\n","  W[idx][:] = wv.word_vec(word)\n","100%|██████████| 43435/43435 [00:00<00:00, 205027.90it/s]\n"]}],"source":["gensim_to_embeddings('%s/processed_full.w2v' %MIMIC_3_DIR, '%s/vocab.csv' %MIMIC_3_DIR,'full')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lPbjLrgqBrzn"},"outputs":[],"source":["from nltk.corpus import stopwords\n","from nltk.tokenize import RegexpTokenizer\n","from tqdm import tqdm\n","\n","def vocab_index_descriptions(vocab_file, vectors_file):\n","    #load lookups\n","    vocab = set()\n","    with open(vocab_file, 'r') as vocabfile:\n","        for i,line in enumerate(vocabfile):\n","            line = line.strip()\n","            if line != '':\n","                vocab.add(line)\n","    ind2w = {i+1:w for i,w in enumerate(sorted(vocab))}\n","    w2ind = {w:i for i,w in ind2w.items()}\n","    desc_dict = load_code_descriptions()\n","        \n","    tokenizer = RegexpTokenizer(r'\\w+')\n","\n","    with open(vectors_file, 'w') as of:\n","        w = csv.writer(of, delimiter=' ')\n","        w.writerow([\"CODE\", \"VECTOR\"])\n","        for code, desc in tqdm(desc_dict.items()):\n","            #same preprocessing steps as in get_discharge_summaries\n","            tokens = [t.lower() for t in tokenizer.tokenize(desc) if not t.isnumeric()]\n","            inds = [w2ind[t] if t in w2ind.keys() else len(w2ind)+1 for t in tokens]\n","            w.writerow([code] + [str(i) for i in inds])\n","\n","\n","def load_code_descriptions(version='mimic3'):\n","    #load description lookup from the appropriate data files\n","    desc_dict = defaultdict(str)\n","    with open(\"%s/D_ICD_DIAGNOSES.csv\" %DATA_DIR, 'r') as descfile:\n","        r = csv.reader(descfile)\n","        #header\n","        next(r)\n","        for row in r:\n","            code = row[1]\n","            desc = row[-1]\n","            desc_dict[reformat(code, True)] = desc\n","    with open(\"%s/D_ICD_PROCEDURES.csv\" %DATA_DIR, 'r') as descfile:\n","        r = csv.reader(descfile)\n","        #header\n","        next(r)\n","        for row in r:\n","            code = row[1]\n","            desc = row[-1]\n","            if code not in desc_dict.keys():\n","                desc_dict[reformat(code, False)] = desc\n","    with open('%s/ICD9_descriptions' %DATA_DIR, 'r') as labelfile:\n","        for i,row in enumerate(labelfile):\n","            row = row.rstrip().split()\n","            code = row[0]\n","            if code not in desc_dict.keys():\n","                desc_dict[code] = ' '.join(row[1:])\n","    return desc_dict\n","\n","def reformat(code, is_diag):\n","    \"\"\"\n","        Put a period in the right place because the MIMIC-3 data files exclude them.\n","        Generally, procedure codes have dots after the first two digits, \n","        while diagnosis codes have dots after the first three digits.\n","    \"\"\"\n","    code = ''.join(code.split('.'))\n","    if is_diag:\n","        if code.startswith('E'):\n","            if len(code) > 4:\n","                code = code[:4] + '.' + code[4:]\n","        else:\n","            if len(code) > 3:\n","                code = code[:3] + '.' + code[3:]\n","    else:\n","        code = code[:2] + '.' + code[2:]\n","    return code"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z2Ht13uiBrzo","outputId":"3c2a3750-834f-46bd-e71d-8db339e11824"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 22267/22267 [00:00<00:00, 136954.03it/s]\n"]}],"source":["vocab_index_descriptions('%s/vocab.csv' % MIMIC_3_DIR,'%s/description_vectors.vocab' %MIMIC_3_DIR)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w4jI8P1HBrzp","outputId":"c58fb269-f756-4279-8f91-0f3a41daf42f"},"outputs":[{"name":"stdout","output_type":"stream","text":["sample_data\n"]}],"source":["!ls"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.2"},"orig_nbformat":4,"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}